## TP+EP

- 只指定TP, 默认ep_size=1, moe权重在intermediate_size这一维度上TP切分
  每张卡拥有所有专家的1/tp分片权重
- 指定TP和EP,且EP=TP,则moe_tp_size = tp_size // ep_size = 1
  则不需要对moe权重在intermediate_size这一维度上TP切分
  每个tp_rank拥有若干个完整的专家权重
- 指定TP和EP,且TP>EP,TP % EP  =0, 则moe_tp_size = tp_size // ep_size
  对于moe权重的每个TP分片进行EP分配
  例如TP16,EP4,moe_tp_size = tp_size // ep_size = 4,  专家权重分成4个TP分片
  每个分片有一个ep_group,将这一分片分配到ep_size张卡上
  例如对于第一个分片,ep_group=[0,4,8,12]
  对于第一份完整的专家,权重被4个ep_group的第一个ep_rank共同拥有
  即第一个moe_tp_group[0,1,2,3]

python\sglang\srt\entrypoints\engine.py

```python
"""
ep_size: 专家维度的并行度,即所有专家被分成ep_size份
即 num_experts_in_ep_group = num_experts // ep_size
ep_group: 按照ep维度切分所有专家的通信组,ep_size即ep_group的大小

ep划分:
当tp_size=ep_size时,只有一个ep_group
表示对所有专家的全量权重ep划分,每个ep_rank会得到完整的若干个专家的权重
当tp_size>ep_size时,有tp_size // ep_size个ep_group
每个ep_group负责所有专家的其中一个分片
表示对所有专家的某个分片权重进行ep划分,每个ep_rank会得到若干个专家的分片权重
第一份专家的完整权重,需要所有ep_group的第一个ep_rank的权重合并才能得到
所以所有ep_group的第一个ep_rank构成一个moe_tp_group
同理所有ep_group的第二个ep_rank构成一个moe_tp_group

moe_tp_group:按照ep切分后的每一份专家,再按tp维度切分的通信组
tp_size // ep_size = 按照ep切分后的每一份专家按tp切分的份数
即tp_size // ep_size = ep_grop的个数 = moe_tp_group的大小
tp_rank // moe_tp_group的大小 = 当前tp_rank属于哪一个moe_tp_group

例如 ep_size=4, tp_size=16
ep_group有[0,4,8,12],[1,5,9,13],[2,6,10,14],[3,7,11,15]
各自处理所有专家的一个tp分片
所有专家被分成4份, 每份由一个moe_tp_group处理, 每个moe_tp_group有4个tp_rank
tp_rank中 0-3为第一个moe_tp_group, 4-7为第二个moe_tp_group

又例如 ep_size=2, tp_size = 16
所有专家被分成2份, 每份由一个moe_tp_group处理, 每个moe_tp_group有8个tp_rank
tp_rank中 0-7为第一个moe_tp_group, 8-15为第二个moe_tp_group

"""
moe_ep_rank = tp_rank // (server_args.tp_size // server_args.ep_size)
```

python\sglang\srt\model_executor\model_runner.py

```python
"""
ep_size 就是专家的并行度 = 所有专家被分成ep_size份 = ep_group的数量
tp_size // ep_size = 每个ep_group里有多少个tp_rank(ep_group的大小)
moe_tp_size = tp_size // ep_size表示ep_group内部的tp并行度
"""
moe_ep_size = ep_size
moe_tp_size = self.tp_size // self.moe_ep_size
```

python\sglang\srt\distributed\parallel_state.py

```python
def initialize_model_parallel(
    tensor_model_parallel_size: int = 1,
    expert_model_parallel_size: int = 1,
    pipeline_model_parallel_size: int = 1,
    backend: Optional[str] = None,
    duplicate_tp_group: bool = False,
) -> None:
    # world size 必须能被 TP×PP 精确描述
    world_size: int = torch.distributed.get_world_size()
    if world_size != tensor_model_parallel_size * pipeline_model_parallel_size:
    raise RuntimeError(...)
    
    """
    构建TP通信组
    每个 TP 组是一个连续区间
    第 0 组 [0 .. tp_size-1]
    第 1 组 [tp_size .. 2*tp_size-1]
    """
    num_tensor_model_parallel_groups: int = world_size // tensor_model_parallel_size
	global _TP
    # 初始化TP Group
    _TP = init_model_parallel_group(
        group_ranks,
        get_world_group().local_rank,
        backend,
        use_message_queue_broadcaster=get_bool_env_var("SGLANG_USE_MESSAGE_QUEUE_BROADCASTER", "true"),
        group_name="tp",
        pynccl_use_current_stream=duplicate_tp_group,
    )
    
    moe_ep_size = expert_model_parallel_size
	moe_tp_size = tensor_model_parallel_size // moe_ep_size
    
    # _MOE_EP按“列”分组（步长是 moe_tp_size）
    global _MOE_EP
    assert _MOE_EP is None
    # ep_size = tp_size 时直接复用tp通信组
    # 即每个tp_rank 分到 num_experts // tp_rank 个完整专家
    if moe_ep_size == tensor_model_parallel_size:
        _MOE_EP = _TP
    else:
        """
        在每个 TP block(大小tp_size)里
        固定j,取 j,j+moe_tp_size,j+2*moe_tp_size, ...，总共取 moe_ep_size 个 rank
        
        tp_size=8, moe_ep_size=2 => moe_tp_size=4
        ep_group有[0,4],[1,5],[2,6],[3,7]
        """
        group_ranks = []
        # 不开pp,num_tensor_model_parallel_groups=1
        for i in range(num_tensor_model_parallel_groups):
            for j in range(moe_tp_size):
                st = i * tensor_model_parallel_size + j
                en = (i + 1) * tensor_model_parallel_size + j
                ranks = list(range(st, en, moe_tp_size))
                group_ranks.append(ranks)
        _MOE_EP = init_model_parallel_group(..., group_name="moe_ep")
        
    # _MOE_TP 按“行”分组（连续块）
	global _MOE_TP
    assert _MOE_TP is None
    if moe_tp_size == tensor_model_parallel_size:
        _MOE_TP = _TP
    else:
        group_ranks = []
        for i in range(num_tensor_model_parallel_groups):
            for j in range(moe_ep_size):
                st = i * tensor_model_parallel_size + j * moe_tp_size
                en = i * tensor_model_parallel_size + (j + 1) * moe_tp_size
                ranks = list(range(st, en))
                group_ranks.append(ranks)
        _MOE_TP = init_model_parallel_group(..., group_name="moe_tp")
```

### Attention

### FusedMoe

init方法

```python
self.num_local_experts = (
    num_experts - num_fused_shared_experts
) // self.moe_ep_size + num_fused_shared_experts

self.intermediate_size_per_partition = intermediate_size // self.moe_tp_size
```

即moe_tp在intermediate_size这一维度上切分

moe_intermediate_size为moe的中间维度
其中w1/w3按dim=0切,w2按dim=1切分

例如qwen3-30B-A3B,config.json中"moe_intermediate_size": 768
其moe权重的shape:

- down_proj.weight [2 048, 768]
- gate_proj.weight [768, 2 048]
- up_proj.weight [768, 2 048]